{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Input, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer, base_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from gensim.models import word2vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = word2vec.KeyedVectors.load_word2vec_format('word2vec/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.data', sep='\\t')\n",
    "test = pd.read_csv('data/test.data', sep='\\t')\n",
    "\n",
    "train.Tokens = [nltk.word_tokenize(sentence.replace('\\\\n', ' ')) for sentence in train.Text]\n",
    "test.Tokens = [nltk.word_tokenize(sentence.replace('\\\\n', ' ')) for sentence in test.Text]\n",
    "\n",
    "train['NewText'] = pd.Series.from_array([' '.join(tokens) for tokens in train.Tokens], train.index)\n",
    "test['NewText'] = pd.Series.from_array([' '.join(tokens) for tokens in test.Tokens], test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.7, max_features=40000, min_df=50,\n",
       "        ngram_range=(2, 6), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='char',\n",
    "                             ngram_range=(2, 6),\n",
    "                             min_df=50,\n",
    "                             max_df=0.7,\n",
    "                             max_features=40000,\n",
    "                             stop_words='english',\n",
    "                             lowercase=True)\n",
    "vectorizer.fit(pd.concat([train.NewText, test.NewText]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<102544x40000 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 237772875 stored elements in Compressed Sparse Row format>,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features = vectorizer.transform(train.NewText)\n",
    "test_features = vectorizer.transform(test.NewText)\n",
    "\n",
    "X_train, X_test = train_features, test_features\n",
    "y_train = np_utils.to_categorical(train.Sentiment, 6)\n",
    "X_train, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(40000,))\n",
    "inp_norm = BatchNormalization(axis=1)(inp)\n",
    "\n",
    "outs = []\n",
    "for i in range(3):\n",
    "    hidden = Dense(512, init='he_uniform', W_regularizer=l2(0.0001), activation='relu')(inp_norm)\n",
    "    batch = BatchNormalization(axis=1)(hidden)\n",
    "    drop = Dropout(0.25)(batch)\n",
    "    hidden = Dense(128, init='he_uniform', W_regularizer=l2(0.0001), activation='relu')(batch)\n",
    "    batch = BatchNormalization(axis=1)(hidden)\n",
    "    drop = Dropout(0.25)(batch)\n",
    "    hidden = Dense(16, init='he_uniform', W_regularizer=l2(0.0001), activation='relu')(drop)\n",
    "    batch = BatchNormalization(axis=1)(hidden)\n",
    "    drop = Dropout(0.5)(batch)\n",
    "    out = Dense(6, init='glorot_uniform', W_regularizer=l2(0.0001), activation='softmax')(drop)\n",
    "    outs.append(out)\n",
    "\n",
    "out = merge(outs, mode='ave')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size):\n",
    "    all_count, i = X.shape[0], 0\n",
    "    num_batchs = all_count // batch_size\n",
    "    shuffle_index = np.arange(all_count)\n",
    "    np.random.shuffle(shuffle_index)\n",
    "    while True:\n",
    "        index_batch = shuffle_index[batch_size * i: batch_size * (i + 1)]\n",
    "        X_batch = X[index_batch,:].todense()\n",
    "        y_batch = y[index_batch,:]\n",
    "        i += 1\n",
    "        yield (np.array(X_batch), y_batch)\n",
    "        if i == num_batchs:\n",
    "            np.random.shuffle(shuffle_index)\n",
    "            i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21\n",
      "272s - loss: 2.3205 - acc: 0.2131\n",
      "Epoch 2/21\n",
      "261s - loss: 2.1150 - acc: 0.3018\n",
      "Epoch 3/21\n",
      "235s - loss: 1.9594 - acc: 0.3738\n",
      "Epoch 4/21\n",
      "253s - loss: 1.8871 - acc: 0.4107\n",
      "Epoch 5/21\n",
      "259s - loss: 1.7916 - acc: 0.4571\n",
      "Epoch 6/21\n",
      "258s - loss: 1.7542 - acc: 0.4773\n",
      "Epoch 7/21\n",
      "258s - loss: 1.6910 - acc: 0.5090\n",
      "Epoch 8/21\n",
      "273s - loss: 1.6587 - acc: 0.5195\n",
      "Epoch 9/21\n",
      "278s - loss: 1.6153 - acc: 0.5410\n",
      "Epoch 10/21\n",
      "288s - loss: 1.5876 - acc: 0.5526\n",
      "Epoch 11/21\n",
      "284s - loss: 1.5431 - acc: 0.5746\n",
      "Epoch 12/21\n",
      "289s - loss: 1.5348 - acc: 0.5734\n",
      "Epoch 13/21\n",
      "288s - loss: 1.5014 - acc: 0.5899\n",
      "Epoch 14/21\n",
      "286s - loss: 1.4824 - acc: 0.5950\n",
      "Epoch 15/21\n",
      "286s - loss: 1.4537 - acc: 0.6094\n",
      "Epoch 16/21\n",
      "271s - loss: 1.4397 - acc: 0.6097\n",
      "Epoch 17/21\n",
      "233s - loss: 1.4150 - acc: 0.6245\n",
      "Epoch 18/21\n",
      "278s - loss: 1.3994 - acc: 0.6312\n",
      "Epoch 19/21\n",
      "280s - loss: 1.3799 - acc: 0.6360\n",
      "Epoch 20/21\n",
      "279s - loss: 1.3660 - acc: 0.6467\n",
      "Epoch 21/21\n",
      "280s - loss: 1.3501 - acc: 0.6493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0b3cb2ae80>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(input=inp, output=out)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "model.fit_generator(generator=batch_generator(X_train, y_train, 1000), \n",
    "                    nb_epoch=21, samples_per_epoch=50000, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Sentiment\n",
      "0   0          5\n",
      "1   1          2\n",
      "2   2          5\n",
      "3   3          5\n",
      "4   4          5\n"
     ]
    }
   ],
   "source": [
    "pred_test = [list(x).index(max(x)) for x in model.predict(X_test.todense())]\n",
    "prediction = pd.DataFrame(data={'Id': test.Id, 'Sentiment': pred_test}, index=test.index)\n",
    "prediction.to_csv('data/prediction.csv', index=False)\n",
    "print(prediction.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
