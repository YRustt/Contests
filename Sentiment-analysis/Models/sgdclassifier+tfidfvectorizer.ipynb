{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow\n",
    "import keras\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim.models import word2vec, doc2vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename, header=0, sep='\\t'):\n",
    "    return pd.read_csv(filename, header=header, sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = read_data('data/train.data')\n",
    "test = read_data('data/test.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102544, 3) (34194, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.Tokens = [nltk.word_tokenize(sentence.replace('\\\\n', ' ')) for sentence in train.Text]\n",
    "test.Tokens = [nltk.word_tokenize(sentence.replace('\\\\n', ' ')) for sentence in test.Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train['NewText'] = pd.Series.from_array([' '.join(tokens) for tokens in train.Tokens], train.index)\n",
    "test['NewText'] = pd.Series.from_array([' '.join(tokens) for tokens in test.Tokens], test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.7, max_features=40000, min_df=50,\n",
       "        ngram_range=(2, 6), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer='char',\n",
    "                             ngram_range=(2, 6),\n",
    "                             min_df=50,\n",
    "                             max_df=0.7,\n",
    "                             max_features=40000,\n",
    "                             stop_words='english',\n",
    "                             lowercase=True)\n",
    "vectorizer.fit(pd.concat([train.NewText, test.NewText]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = vectorizer.transform(train.NewText)\n",
    "test_features = vectorizer.transform(test.NewText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(train_features, train.Sentiment.values, test_size=0.3)\n",
    "X_test = test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  0.703761493452\n",
      "Valid:  0.589487712911\n"
     ]
    }
   ],
   "source": [
    "sgd_est = SGDClassifier(loss='modified_huber', penalty='l2', n_iter=5, n_jobs=10)\n",
    "sgd_est.fit(X_train, y_train)\n",
    "print('Train: ', sgd_est.score(X_train, y_train))\n",
    "print('Valid: ', sgd_est.score(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id  Sentiment\n",
      "0   0          5\n",
      "1   1          2\n",
      "2   2          5\n",
      "3   3          5\n",
      "4   4          5\n"
     ]
    }
   ],
   "source": [
    "pred_test = sgd_est.predict(X_test)\n",
    "prediction = pd.DataFrame(data={'Id': test.Id, 'Sentiment': pred_test}, index=test.index)\n",
    "prediction.to_csv('data/prediction.csv', index=False)\n",
    "print(prediction.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
